{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 30pt; font-weight: bold; margin: 1em 0em 1em 0em\">Auto-Encoders v. Variational Auto-Encoders</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T20:50:57.082393Z",
     "start_time": "2018-11-12T20:50:57.077731Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T20:50:57.253920Z",
     "start_time": "2018-11-12T20:50:57.250539Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../autoencoders'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T20:51:00.016098Z",
     "start_time": "2018-11-12T20:50:58.206408Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"Magic\" commands for automatic reloading of module, perfect for prototyping\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:21:31.980961Z",
     "start_time": "2018-11-12T21:21:31.956034Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T20:51:02.618381Z",
     "start_time": "2018-11-12T20:51:02.514859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> total trainning batch number: 600\n",
      ">> total testing batch number: 100\n"
     ]
    }
   ],
   "source": [
    "root = '../data'\n",
    "    \n",
    "trans = transforms.Compose([\n",
    "    # transforms.ToTensor(), \n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.Resize((8, 8)),\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize((0.5,), (1.0,))\n",
    "])\n",
    "\n",
    "# if not exist, download mnist dataset\n",
    "train_set = datasets.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = datasets.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "print('>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T03:45:22.991142Z",
     "start_time": "2018-11-12T03:45:22.970512Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=12, bias=True)\n",
      "  (fc4): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (fc5): Linear(in_features=2, out_features=12, bias=True)\n",
      "  (fc6): Linear(in_features=12, out_features=50, bias=True)\n",
      "  (fc7): Linear(in_features=50, out_features=128, bias=True)\n",
      "  (fc8): Linear(in_features=128, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = autoencoders.AutoEncoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Loss and training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T03:45:23.614079Z",
     "start_time": "2018-11-12T03:45:23.592076Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T03:45:23.772226Z",
     "start_time": "2018-11-12T03:45:23.744774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T03:45:24.199563Z",
     "start_time": "2018-11-12T03:45:24.176310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T03:45:24.353173Z",
     "start_time": "2018-11-12T03:45:24.335687Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T03:45:24.547449Z",
     "start_time": "2018-11-12T03:45:24.514126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x):\n",
    "    \n",
    "    # reconstruction = F.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')\n",
    "    reconstruction = F.mse_loss(recon_x, x.view(-1, 28*28), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return reconstruction\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch = model(data)\n",
    "        loss = loss_function(recon_batch, data)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('>> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch = model(data)\n",
    "            test_loss += loss_function(recon_batch, data).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(100, 1, 28, 28)[:n]])\n",
    "                utils.save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('>> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T03:47:34.029605Z",
     "start_time": "2018-11-12T03:47:33.318279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch = model(data)\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(100, 1, 28, 28)[:n]])\n",
    "                utils.save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_0.png', nrow=n)\n",
    "                \n",
    "    sample = 3 * torch.randn(64, 2).to(device)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    utils.save_image(sample.view(64, 1, 28, 28),\n",
    "               'results/sample_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T04:06:39.479411Z",
     "start_time": "2018-11-12T04:06:39.456451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T04:40:20.108788Z",
     "start_time": "2018-11-12T04:40:20.086669Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "space = np.array([[x, y] for y in np.linspace(0, 5, 16) for x in np.linspace(0, 5, 16)], dtype=np.float)\n",
    "space = torch.from_numpy(space).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T04:40:20.992388Z",
     "start_time": "2018-11-12T04:40:20.919874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epoch += 1\n",
    "with torch.no_grad():\n",
    "    # sample = torch.randn(64, 2).to(device)\n",
    "    sample = space\n",
    "    sample = model.decode(sample).cpu()\n",
    "    utils.save_image(sample.view(16 * 16, 1, 28, 28),\n",
    "               'results/sample_' + str(epoch) + '.png', nrow=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T05:06:57.911177Z",
     "start_time": "2018-11-12T04:41:14.606311Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 100 [0/60000 (0%)]\tLoss: 27.434194\n",
      "Train Epoch: 100 [20000/60000 (33%)]\tLoss: 27.653030\n",
      "Train Epoch: 100 [40000/60000 (67%)]\tLoss: 28.132900\n",
      ">> Epoch: 100 Average loss: 28.0925\n",
      ">> Test set loss: 29.1073\n",
      "Train Epoch: 101 [0/60000 (0%)]\tLoss: 26.247185\n",
      "Train Epoch: 101 [20000/60000 (33%)]\tLoss: 26.054910\n",
      "Train Epoch: 101 [40000/60000 (67%)]\tLoss: 27.263174\n",
      ">> Epoch: 101 Average loss: 27.8610\n",
      ">> Test set loss: 28.4108\n",
      "Train Epoch: 102 [0/60000 (0%)]\tLoss: 26.615098\n",
      "Train Epoch: 102 [20000/60000 (33%)]\tLoss: 25.315955\n",
      "Train Epoch: 102 [40000/60000 (67%)]\tLoss: 27.422822\n",
      ">> Epoch: 102 Average loss: 27.5723\n",
      ">> Test set loss: 27.8513\n",
      "Train Epoch: 103 [0/60000 (0%)]\tLoss: 29.024128\n",
      "Train Epoch: 103 [20000/60000 (33%)]\tLoss: 27.392444\n",
      "Train Epoch: 103 [40000/60000 (67%)]\tLoss: 27.526680\n",
      ">> Epoch: 103 Average loss: 27.5940\n",
      ">> Test set loss: 27.8839\n",
      "Train Epoch: 104 [0/60000 (0%)]\tLoss: 24.922517\n",
      "Train Epoch: 104 [20000/60000 (33%)]\tLoss: 26.165518\n",
      "Train Epoch: 104 [40000/60000 (67%)]\tLoss: 26.273813\n",
      ">> Epoch: 104 Average loss: 27.4592\n",
      ">> Test set loss: 27.8237\n",
      "Train Epoch: 105 [0/60000 (0%)]\tLoss: 27.129673\n",
      "Train Epoch: 105 [20000/60000 (33%)]\tLoss: 28.588418\n",
      "Train Epoch: 105 [40000/60000 (67%)]\tLoss: 27.597593\n",
      ">> Epoch: 105 Average loss: 27.4738\n",
      ">> Test set loss: 27.8678\n",
      "Train Epoch: 106 [0/60000 (0%)]\tLoss: 24.843801\n",
      "Train Epoch: 106 [20000/60000 (33%)]\tLoss: 27.897429\n",
      "Train Epoch: 106 [40000/60000 (67%)]\tLoss: 28.165466\n",
      ">> Epoch: 106 Average loss: 27.3777\n",
      ">> Test set loss: 27.7388\n",
      "Train Epoch: 107 [0/60000 (0%)]\tLoss: 28.570010\n",
      "Train Epoch: 107 [20000/60000 (33%)]\tLoss: 28.103264\n",
      "Train Epoch: 107 [40000/60000 (67%)]\tLoss: 27.515374\n",
      ">> Epoch: 107 Average loss: 27.2969\n",
      ">> Test set loss: 27.8645\n",
      "Train Epoch: 108 [0/60000 (0%)]\tLoss: 26.511135\n",
      "Train Epoch: 108 [20000/60000 (33%)]\tLoss: 30.413240\n",
      "Train Epoch: 108 [40000/60000 (67%)]\tLoss: 27.758560\n",
      ">> Epoch: 108 Average loss: 27.3988\n",
      ">> Test set loss: 27.8786\n",
      "Train Epoch: 109 [0/60000 (0%)]\tLoss: 28.052671\n",
      "Train Epoch: 109 [20000/60000 (33%)]\tLoss: 25.806895\n",
      "Train Epoch: 109 [40000/60000 (67%)]\tLoss: 28.681379\n",
      ">> Epoch: 109 Average loss: 27.2711\n",
      ">> Test set loss: 27.6674\n",
      "Train Epoch: 110 [0/60000 (0%)]\tLoss: 26.941206\n",
      "Train Epoch: 110 [20000/60000 (33%)]\tLoss: 24.575403\n",
      "Train Epoch: 110 [40000/60000 (67%)]\tLoss: 28.885322\n",
      ">> Epoch: 110 Average loss: 27.2504\n",
      ">> Test set loss: 27.8331\n",
      "Train Epoch: 111 [0/60000 (0%)]\tLoss: 30.179963\n",
      "Train Epoch: 111 [20000/60000 (33%)]\tLoss: 26.454099\n",
      "Train Epoch: 111 [40000/60000 (67%)]\tLoss: 26.208752\n",
      ">> Epoch: 111 Average loss: 27.4583\n",
      ">> Test set loss: 27.7299\n",
      "Train Epoch: 112 [0/60000 (0%)]\tLoss: 25.929431\n",
      "Train Epoch: 112 [20000/60000 (33%)]\tLoss: 27.607209\n",
      "Train Epoch: 112 [40000/60000 (67%)]\tLoss: 27.886936\n",
      ">> Epoch: 112 Average loss: 27.2916\n",
      ">> Test set loss: 27.8134\n",
      "Train Epoch: 113 [0/60000 (0%)]\tLoss: 27.242886\n",
      "Train Epoch: 113 [20000/60000 (33%)]\tLoss: 29.263879\n",
      "Train Epoch: 113 [40000/60000 (67%)]\tLoss: 29.338188\n",
      ">> Epoch: 113 Average loss: 27.3659\n",
      ">> Test set loss: 27.7088\n",
      "Train Epoch: 114 [0/60000 (0%)]\tLoss: 25.198511\n",
      "Train Epoch: 114 [20000/60000 (33%)]\tLoss: 24.589673\n",
      "Train Epoch: 114 [40000/60000 (67%)]\tLoss: 26.222288\n",
      ">> Epoch: 114 Average loss: 27.3040\n",
      ">> Test set loss: 27.7145\n",
      "Train Epoch: 115 [0/60000 (0%)]\tLoss: 25.833359\n",
      "Train Epoch: 115 [20000/60000 (33%)]\tLoss: 27.765754\n",
      "Train Epoch: 115 [40000/60000 (67%)]\tLoss: 26.129612\n",
      ">> Epoch: 115 Average loss: 27.3061\n",
      ">> Test set loss: 27.7898\n",
      "Train Epoch: 116 [0/60000 (0%)]\tLoss: 25.582041\n",
      "Train Epoch: 116 [20000/60000 (33%)]\tLoss: 28.875232\n",
      "Train Epoch: 116 [40000/60000 (67%)]\tLoss: 26.892922\n",
      ">> Epoch: 116 Average loss: 27.2886\n",
      ">> Test set loss: 27.6140\n",
      "Train Epoch: 117 [0/60000 (0%)]\tLoss: 26.786970\n",
      "Train Epoch: 117 [20000/60000 (33%)]\tLoss: 26.601733\n",
      "Train Epoch: 117 [40000/60000 (67%)]\tLoss: 28.803086\n",
      ">> Epoch: 117 Average loss: 27.2813\n",
      ">> Test set loss: 27.7771\n",
      "Train Epoch: 118 [0/60000 (0%)]\tLoss: 26.081377\n",
      "Train Epoch: 118 [20000/60000 (33%)]\tLoss: 25.034529\n",
      "Train Epoch: 118 [40000/60000 (67%)]\tLoss: 27.770068\n",
      ">> Epoch: 118 Average loss: 27.1909\n",
      ">> Test set loss: 27.6997\n",
      "Train Epoch: 119 [0/60000 (0%)]\tLoss: 28.358259\n",
      "Train Epoch: 119 [20000/60000 (33%)]\tLoss: 26.808013\n",
      "Train Epoch: 119 [40000/60000 (67%)]\tLoss: 28.002012\n",
      ">> Epoch: 119 Average loss: 27.2064\n",
      ">> Test set loss: 27.8904\n",
      "Train Epoch: 120 [0/60000 (0%)]\tLoss: 28.178591\n",
      "Train Epoch: 120 [20000/60000 (33%)]\tLoss: 27.196523\n",
      "Train Epoch: 120 [40000/60000 (67%)]\tLoss: 26.248086\n",
      ">> Epoch: 120 Average loss: 27.2027\n",
      ">> Test set loss: 27.6412\n",
      "Train Epoch: 121 [0/60000 (0%)]\tLoss: 26.756013\n",
      "Train Epoch: 121 [20000/60000 (33%)]\tLoss: 28.626311\n",
      "Train Epoch: 121 [40000/60000 (67%)]\tLoss: 31.856216\n",
      ">> Epoch: 121 Average loss: 27.1374\n",
      ">> Test set loss: 27.6922\n",
      "Train Epoch: 122 [0/60000 (0%)]\tLoss: 28.169260\n",
      "Train Epoch: 122 [20000/60000 (33%)]\tLoss: 29.889041\n",
      "Train Epoch: 122 [40000/60000 (67%)]\tLoss: 29.112227\n",
      ">> Epoch: 122 Average loss: 27.2187\n",
      ">> Test set loss: 27.8536\n",
      "Train Epoch: 123 [0/60000 (0%)]\tLoss: 24.830732\n",
      "Train Epoch: 123 [20000/60000 (33%)]\tLoss: 25.456345\n",
      "Train Epoch: 123 [40000/60000 (67%)]\tLoss: 30.412019\n",
      ">> Epoch: 123 Average loss: 27.1914\n",
      ">> Test set loss: 27.6874\n",
      "Train Epoch: 124 [0/60000 (0%)]\tLoss: 27.612554\n",
      "Train Epoch: 124 [20000/60000 (33%)]\tLoss: 27.649719\n",
      "Train Epoch: 124 [40000/60000 (67%)]\tLoss: 26.862935\n",
      ">> Epoch: 124 Average loss: 27.2410\n",
      ">> Test set loss: 27.7532\n",
      "Train Epoch: 125 [0/60000 (0%)]\tLoss: 28.421367\n",
      "Train Epoch: 125 [20000/60000 (33%)]\tLoss: 27.863208\n",
      "Train Epoch: 125 [40000/60000 (67%)]\tLoss: 24.949717\n",
      ">> Epoch: 125 Average loss: 27.1816\n",
      ">> Test set loss: 27.6270\n",
      "Train Epoch: 126 [0/60000 (0%)]\tLoss: 26.552332\n",
      "Train Epoch: 126 [20000/60000 (33%)]\tLoss: 24.344929\n",
      "Train Epoch: 126 [40000/60000 (67%)]\tLoss: 27.404221\n",
      ">> Epoch: 126 Average loss: 27.1405\n",
      ">> Test set loss: 27.6208\n",
      "Train Epoch: 127 [0/60000 (0%)]\tLoss: 28.108452\n",
      "Train Epoch: 127 [20000/60000 (33%)]\tLoss: 27.689688\n",
      "Train Epoch: 127 [40000/60000 (67%)]\tLoss: 26.506228\n",
      ">> Epoch: 127 Average loss: 27.1936\n",
      ">> Test set loss: 27.5713\n",
      "Train Epoch: 128 [0/60000 (0%)]\tLoss: 26.099258\n",
      "Train Epoch: 128 [20000/60000 (33%)]\tLoss: 28.553325\n",
      "Train Epoch: 128 [40000/60000 (67%)]\tLoss: 27.253608\n",
      ">> Epoch: 128 Average loss: 27.2051\n",
      ">> Test set loss: 27.8006\n",
      "Train Epoch: 129 [0/60000 (0%)]\tLoss: 30.853916\n",
      "Train Epoch: 129 [20000/60000 (33%)]\tLoss: 28.370593\n",
      "Train Epoch: 129 [40000/60000 (67%)]\tLoss: 28.074785\n",
      ">> Epoch: 129 Average loss: 27.2590\n",
      ">> Test set loss: 27.7698\n",
      "Train Epoch: 130 [0/60000 (0%)]\tLoss: 26.786633\n",
      "Train Epoch: 130 [20000/60000 (33%)]\tLoss: 27.417798\n",
      "Train Epoch: 130 [40000/60000 (67%)]\tLoss: 28.784072\n",
      ">> Epoch: 130 Average loss: 27.2928\n",
      ">> Test set loss: 28.0336\n",
      "Train Epoch: 131 [0/60000 (0%)]\tLoss: 27.801216\n",
      "Train Epoch: 131 [20000/60000 (33%)]\tLoss: 27.803013\n",
      "Train Epoch: 131 [40000/60000 (67%)]\tLoss: 28.205269\n",
      ">> Epoch: 131 Average loss: 27.9147\n",
      ">> Test set loss: 27.8779\n",
      "Train Epoch: 132 [0/60000 (0%)]\tLoss: 26.043323\n",
      "Train Epoch: 132 [20000/60000 (33%)]\tLoss: 28.747229\n",
      "Train Epoch: 132 [40000/60000 (67%)]\tLoss: 26.935354\n",
      ">> Epoch: 132 Average loss: 27.4716\n",
      ">> Test set loss: 28.0970\n",
      "Train Epoch: 133 [0/60000 (0%)]\tLoss: 25.925496\n",
      "Train Epoch: 133 [20000/60000 (33%)]\tLoss: 30.944688\n",
      "Train Epoch: 133 [40000/60000 (67%)]\tLoss: 28.545974\n",
      ">> Epoch: 133 Average loss: 27.5484\n",
      ">> Test set loss: 27.8346\n",
      "Train Epoch: 134 [0/60000 (0%)]\tLoss: 27.436875\n",
      "Train Epoch: 134 [20000/60000 (33%)]\tLoss: 26.981624\n",
      "Train Epoch: 134 [40000/60000 (67%)]\tLoss: 27.738291\n",
      ">> Epoch: 134 Average loss: 27.2874\n",
      ">> Test set loss: 27.7245\n",
      "Train Epoch: 135 [0/60000 (0%)]\tLoss: 27.820627\n",
      "Train Epoch: 135 [20000/60000 (33%)]\tLoss: 25.444890\n",
      "Train Epoch: 135 [40000/60000 (67%)]\tLoss: 27.098672\n",
      ">> Epoch: 135 Average loss: 27.2594\n",
      ">> Test set loss: 27.8932\n",
      "Train Epoch: 136 [0/60000 (0%)]\tLoss: 27.330381\n",
      "Train Epoch: 136 [20000/60000 (33%)]\tLoss: 27.648157\n",
      "Train Epoch: 136 [40000/60000 (67%)]\tLoss: 27.470759\n",
      ">> Epoch: 136 Average loss: 27.3341\n",
      ">> Test set loss: 27.8252\n",
      "Train Epoch: 137 [0/60000 (0%)]\tLoss: 31.415994\n",
      "Train Epoch: 137 [20000/60000 (33%)]\tLoss: 25.098523\n",
      "Train Epoch: 137 [40000/60000 (67%)]\tLoss: 27.863030\n",
      ">> Epoch: 137 Average loss: 27.2296\n",
      ">> Test set loss: 27.5839\n",
      "Train Epoch: 138 [0/60000 (0%)]\tLoss: 28.017043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 138 [20000/60000 (33%)]\tLoss: 28.478911\n",
      "Train Epoch: 138 [40000/60000 (67%)]\tLoss: 28.039736\n",
      ">> Epoch: 138 Average loss: 27.2994\n",
      ">> Test set loss: 27.8493\n",
      "Train Epoch: 139 [0/60000 (0%)]\tLoss: 26.656230\n",
      "Train Epoch: 139 [20000/60000 (33%)]\tLoss: 26.371052\n",
      "Train Epoch: 139 [40000/60000 (67%)]\tLoss: 28.662974\n",
      ">> Epoch: 139 Average loss: 27.3739\n",
      ">> Test set loss: 27.8157\n",
      "Train Epoch: 140 [0/60000 (0%)]\tLoss: 25.846450\n",
      "Train Epoch: 140 [20000/60000 (33%)]\tLoss: 28.036323\n",
      "Train Epoch: 140 [40000/60000 (67%)]\tLoss: 27.240313\n",
      ">> Epoch: 140 Average loss: 27.2803\n",
      ">> Test set loss: 27.7956\n",
      "Train Epoch: 141 [0/60000 (0%)]\tLoss: 29.586353\n",
      "Train Epoch: 141 [20000/60000 (33%)]\tLoss: 26.635659\n",
      "Train Epoch: 141 [40000/60000 (67%)]\tLoss: 26.787249\n",
      ">> Epoch: 141 Average loss: 27.7083\n",
      ">> Test set loss: 27.9262\n",
      "Train Epoch: 142 [0/60000 (0%)]\tLoss: 26.102864\n",
      "Train Epoch: 142 [20000/60000 (33%)]\tLoss: 25.811187\n",
      "Train Epoch: 142 [40000/60000 (67%)]\tLoss: 26.763083\n",
      ">> Epoch: 142 Average loss: 27.2205\n",
      ">> Test set loss: 27.9210\n",
      "Train Epoch: 143 [0/60000 (0%)]\tLoss: 28.950200\n",
      "Train Epoch: 143 [20000/60000 (33%)]\tLoss: 25.082947\n",
      "Train Epoch: 143 [40000/60000 (67%)]\tLoss: 28.002200\n",
      ">> Epoch: 143 Average loss: 27.1406\n",
      ">> Test set loss: 27.5399\n",
      "Train Epoch: 144 [0/60000 (0%)]\tLoss: 27.194014\n",
      "Train Epoch: 144 [20000/60000 (33%)]\tLoss: 27.830872\n",
      "Train Epoch: 144 [40000/60000 (67%)]\tLoss: 28.247122\n",
      ">> Epoch: 144 Average loss: 27.2417\n",
      ">> Test set loss: 27.6991\n",
      "Train Epoch: 145 [0/60000 (0%)]\tLoss: 27.787957\n",
      "Train Epoch: 145 [20000/60000 (33%)]\tLoss: 26.740981\n",
      "Train Epoch: 145 [40000/60000 (67%)]\tLoss: 26.190483\n",
      ">> Epoch: 145 Average loss: 27.2332\n",
      ">> Test set loss: 27.7731\n",
      "Train Epoch: 146 [0/60000 (0%)]\tLoss: 26.752778\n",
      "Train Epoch: 146 [20000/60000 (33%)]\tLoss: 28.704199\n",
      "Train Epoch: 146 [40000/60000 (67%)]\tLoss: 28.055583\n",
      ">> Epoch: 146 Average loss: 27.0897\n",
      ">> Test set loss: 27.5159\n",
      "Train Epoch: 147 [0/60000 (0%)]\tLoss: 27.814629\n",
      "Train Epoch: 147 [20000/60000 (33%)]\tLoss: 27.562617\n",
      "Train Epoch: 147 [40000/60000 (67%)]\tLoss: 25.925576\n",
      ">> Epoch: 147 Average loss: 27.1091\n",
      ">> Test set loss: 27.5932\n",
      "Train Epoch: 148 [0/60000 (0%)]\tLoss: 28.556907\n",
      "Train Epoch: 148 [20000/60000 (33%)]\tLoss: 24.181409\n",
      "Train Epoch: 148 [40000/60000 (67%)]\tLoss: 26.456724\n",
      ">> Epoch: 148 Average loss: 27.0706\n",
      ">> Test set loss: 27.4927\n",
      "Train Epoch: 149 [0/60000 (0%)]\tLoss: 25.496404\n",
      "Train Epoch: 149 [20000/60000 (33%)]\tLoss: 28.123813\n",
      "Train Epoch: 149 [40000/60000 (67%)]\tLoss: 28.585730\n",
      ">> Epoch: 149 Average loss: 27.0501\n",
      ">> Test set loss: 27.4842\n",
      "Train Epoch: 150 [0/60000 (0%)]\tLoss: 26.981211\n",
      "Train Epoch: 150 [20000/60000 (33%)]\tLoss: 24.851616\n",
      "Train Epoch: 150 [40000/60000 (67%)]\tLoss: 27.793372\n",
      ">> Epoch: 150 Average loss: 27.1112\n",
      ">> Test set loss: 27.5175\n",
      "Train Epoch: 151 [0/60000 (0%)]\tLoss: 26.303745\n",
      "Train Epoch: 151 [20000/60000 (33%)]\tLoss: 26.545732\n",
      "Train Epoch: 151 [40000/60000 (67%)]\tLoss: 27.891331\n",
      ">> Epoch: 151 Average loss: 27.1166\n",
      ">> Test set loss: 27.5081\n",
      "Train Epoch: 152 [0/60000 (0%)]\tLoss: 26.611438\n",
      "Train Epoch: 152 [20000/60000 (33%)]\tLoss: 26.507607\n",
      "Train Epoch: 152 [40000/60000 (67%)]\tLoss: 27.408638\n",
      ">> Epoch: 152 Average loss: 27.1016\n",
      ">> Test set loss: 27.4090\n",
      "Train Epoch: 153 [0/60000 (0%)]\tLoss: 25.342991\n",
      "Train Epoch: 153 [20000/60000 (33%)]\tLoss: 27.452747\n",
      "Train Epoch: 153 [40000/60000 (67%)]\tLoss: 28.746487\n",
      ">> Epoch: 153 Average loss: 27.0550\n",
      ">> Test set loss: 27.5480\n",
      "Train Epoch: 154 [0/60000 (0%)]\tLoss: 28.728291\n",
      "Train Epoch: 154 [20000/60000 (33%)]\tLoss: 28.125437\n",
      "Train Epoch: 154 [40000/60000 (67%)]\tLoss: 27.837751\n",
      ">> Epoch: 154 Average loss: 26.9666\n",
      ">> Test set loss: 27.4539\n",
      "Train Epoch: 155 [0/60000 (0%)]\tLoss: 27.597036\n",
      "Train Epoch: 155 [20000/60000 (33%)]\tLoss: 26.133857\n",
      "Train Epoch: 155 [40000/60000 (67%)]\tLoss: 25.364331\n",
      ">> Epoch: 155 Average loss: 26.9199\n",
      ">> Test set loss: 27.4786\n",
      "Train Epoch: 156 [0/60000 (0%)]\tLoss: 28.164561\n",
      "Train Epoch: 156 [20000/60000 (33%)]\tLoss: 24.469087\n",
      "Train Epoch: 156 [40000/60000 (67%)]\tLoss: 28.087119\n",
      ">> Epoch: 156 Average loss: 26.9874\n",
      ">> Test set loss: 27.4393\n",
      "Train Epoch: 157 [0/60000 (0%)]\tLoss: 26.640764\n",
      "Train Epoch: 157 [20000/60000 (33%)]\tLoss: 25.466023\n",
      "Train Epoch: 157 [40000/60000 (67%)]\tLoss: 26.355591\n",
      ">> Epoch: 157 Average loss: 26.9635\n",
      ">> Test set loss: 27.5096\n",
      "Train Epoch: 158 [0/60000 (0%)]\tLoss: 26.948245\n",
      "Train Epoch: 158 [20000/60000 (33%)]\tLoss: 29.335840\n",
      "Train Epoch: 158 [40000/60000 (67%)]\tLoss: 27.828831\n",
      ">> Epoch: 158 Average loss: 27.1252\n",
      ">> Test set loss: 27.6022\n",
      "Train Epoch: 159 [0/60000 (0%)]\tLoss: 23.688210\n",
      "Train Epoch: 159 [20000/60000 (33%)]\tLoss: 27.393618\n",
      "Train Epoch: 159 [40000/60000 (67%)]\tLoss: 27.888115\n",
      ">> Epoch: 159 Average loss: 27.1444\n",
      ">> Test set loss: 27.4597\n",
      "Train Epoch: 160 [0/60000 (0%)]\tLoss: 26.317371\n",
      "Train Epoch: 160 [20000/60000 (33%)]\tLoss: 28.510281\n",
      "Train Epoch: 160 [40000/60000 (67%)]\tLoss: 24.768535\n",
      ">> Epoch: 160 Average loss: 27.0365\n",
      ">> Test set loss: 27.5826\n",
      "Train Epoch: 161 [0/60000 (0%)]\tLoss: 26.539534\n",
      "Train Epoch: 161 [20000/60000 (33%)]\tLoss: 30.369849\n",
      "Train Epoch: 161 [40000/60000 (67%)]\tLoss: 27.476917\n",
      ">> Epoch: 161 Average loss: 27.1079\n",
      ">> Test set loss: 27.5145\n",
      "Train Epoch: 162 [0/60000 (0%)]\tLoss: 27.801721\n",
      "Train Epoch: 162 [20000/60000 (33%)]\tLoss: 28.455828\n",
      "Train Epoch: 162 [40000/60000 (67%)]\tLoss: 28.045972\n",
      ">> Epoch: 162 Average loss: 27.1538\n",
      ">> Test set loss: 27.6886\n",
      "Train Epoch: 163 [0/60000 (0%)]\tLoss: 26.641489\n",
      "Train Epoch: 163 [20000/60000 (33%)]\tLoss: 22.947754\n",
      "Train Epoch: 163 [40000/60000 (67%)]\tLoss: 27.551858\n",
      ">> Epoch: 163 Average loss: 27.2272\n",
      ">> Test set loss: 27.6247\n",
      "Train Epoch: 164 [0/60000 (0%)]\tLoss: 26.012593\n",
      "Train Epoch: 164 [20000/60000 (33%)]\tLoss: 29.035254\n",
      "Train Epoch: 164 [40000/60000 (67%)]\tLoss: 27.105107\n",
      ">> Epoch: 164 Average loss: 27.0465\n",
      ">> Test set loss: 27.5147\n",
      "Train Epoch: 165 [0/60000 (0%)]\tLoss: 28.593530\n",
      "Train Epoch: 165 [20000/60000 (33%)]\tLoss: 28.250027\n",
      "Train Epoch: 165 [40000/60000 (67%)]\tLoss: 28.763110\n",
      ">> Epoch: 165 Average loss: 26.9234\n",
      ">> Test set loss: 27.4709\n",
      "Train Epoch: 166 [0/60000 (0%)]\tLoss: 25.087471\n",
      "Train Epoch: 166 [20000/60000 (33%)]\tLoss: 28.026729\n",
      "Train Epoch: 166 [40000/60000 (67%)]\tLoss: 25.655742\n",
      ">> Epoch: 166 Average loss: 26.9793\n",
      ">> Test set loss: 27.4336\n",
      "Train Epoch: 167 [0/60000 (0%)]\tLoss: 29.041299\n",
      "Train Epoch: 167 [20000/60000 (33%)]\tLoss: 24.879238\n",
      "Train Epoch: 167 [40000/60000 (67%)]\tLoss: 26.776426\n",
      ">> Epoch: 167 Average loss: 27.0545\n",
      ">> Test set loss: 27.5407\n",
      "Train Epoch: 168 [0/60000 (0%)]\tLoss: 27.853679\n",
      "Train Epoch: 168 [20000/60000 (33%)]\tLoss: 27.545537\n",
      "Train Epoch: 168 [40000/60000 (67%)]\tLoss: 27.392075\n",
      ">> Epoch: 168 Average loss: 27.0805\n",
      ">> Test set loss: 27.4925\n",
      "Train Epoch: 169 [0/60000 (0%)]\tLoss: 25.394788\n",
      "Train Epoch: 169 [20000/60000 (33%)]\tLoss: 24.377036\n",
      "Train Epoch: 169 [40000/60000 (67%)]\tLoss: 27.981028\n",
      ">> Epoch: 169 Average loss: 27.1023\n",
      ">> Test set loss: 27.4869\n",
      "Train Epoch: 170 [0/60000 (0%)]\tLoss: 26.685496\n",
      "Train Epoch: 170 [20000/60000 (33%)]\tLoss: 27.871660\n",
      "Train Epoch: 170 [40000/60000 (67%)]\tLoss: 27.780618\n",
      ">> Epoch: 170 Average loss: 26.9734\n",
      ">> Test set loss: 27.4215\n",
      "Train Epoch: 171 [0/60000 (0%)]\tLoss: 27.363333\n",
      "Train Epoch: 171 [20000/60000 (33%)]\tLoss: 26.064541\n",
      "Train Epoch: 171 [40000/60000 (67%)]\tLoss: 27.383457\n",
      ">> Epoch: 171 Average loss: 26.9825\n",
      ">> Test set loss: 27.4587\n",
      "Train Epoch: 172 [0/60000 (0%)]\tLoss: 27.641260\n",
      "Train Epoch: 172 [20000/60000 (33%)]\tLoss: 26.302214\n",
      "Train Epoch: 172 [40000/60000 (67%)]\tLoss: 24.901289\n",
      ">> Epoch: 172 Average loss: 26.9974\n",
      ">> Test set loss: 27.4421\n",
      "Train Epoch: 173 [0/60000 (0%)]\tLoss: 27.291565\n",
      "Train Epoch: 173 [20000/60000 (33%)]\tLoss: 26.383308\n",
      "Train Epoch: 173 [40000/60000 (67%)]\tLoss: 27.157192\n",
      ">> Epoch: 173 Average loss: 26.9449\n",
      ">> Test set loss: 27.3715\n",
      "Train Epoch: 174 [0/60000 (0%)]\tLoss: 28.741323\n",
      "Train Epoch: 174 [20000/60000 (33%)]\tLoss: 26.680974\n",
      "Train Epoch: 174 [40000/60000 (67%)]\tLoss: 27.247151\n",
      ">> Epoch: 174 Average loss: 26.8986\n",
      ">> Test set loss: 27.5265\n",
      "Train Epoch: 175 [0/60000 (0%)]\tLoss: 28.263418\n",
      "Train Epoch: 175 [20000/60000 (33%)]\tLoss: 23.895950\n",
      "Train Epoch: 175 [40000/60000 (67%)]\tLoss: 25.441755\n",
      ">> Epoch: 175 Average loss: 26.9878\n",
      ">> Test set loss: 27.5585\n",
      "Train Epoch: 176 [0/60000 (0%)]\tLoss: 24.937434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 176 [20000/60000 (33%)]\tLoss: 25.644033\n",
      "Train Epoch: 176 [40000/60000 (67%)]\tLoss: 26.782109\n",
      ">> Epoch: 176 Average loss: 26.9754\n",
      ">> Test set loss: 27.5705\n",
      "Train Epoch: 177 [0/60000 (0%)]\tLoss: 26.766528\n",
      "Train Epoch: 177 [20000/60000 (33%)]\tLoss: 29.254583\n",
      "Train Epoch: 177 [40000/60000 (67%)]\tLoss: 23.948616\n",
      ">> Epoch: 177 Average loss: 26.9804\n",
      ">> Test set loss: 27.3789\n",
      "Train Epoch: 178 [0/60000 (0%)]\tLoss: 25.275842\n",
      "Train Epoch: 178 [20000/60000 (33%)]\tLoss: 25.949575\n",
      "Train Epoch: 178 [40000/60000 (67%)]\tLoss: 27.896028\n",
      ">> Epoch: 178 Average loss: 26.8892\n",
      ">> Test set loss: 27.7058\n",
      "Train Epoch: 179 [0/60000 (0%)]\tLoss: 26.909988\n",
      "Train Epoch: 179 [20000/60000 (33%)]\tLoss: 26.633896\n",
      "Train Epoch: 179 [40000/60000 (67%)]\tLoss: 25.283594\n",
      ">> Epoch: 179 Average loss: 26.9147\n",
      ">> Test set loss: 27.4598\n",
      "Train Epoch: 180 [0/60000 (0%)]\tLoss: 26.766208\n",
      "Train Epoch: 180 [20000/60000 (33%)]\tLoss: 28.998052\n",
      "Train Epoch: 180 [40000/60000 (67%)]\tLoss: 28.345520\n",
      ">> Epoch: 180 Average loss: 27.0304\n",
      ">> Test set loss: 27.3528\n",
      "Train Epoch: 181 [0/60000 (0%)]\tLoss: 26.704944\n",
      "Train Epoch: 181 [20000/60000 (33%)]\tLoss: 27.078074\n",
      "Train Epoch: 181 [40000/60000 (67%)]\tLoss: 26.255076\n",
      ">> Epoch: 181 Average loss: 26.8897\n",
      ">> Test set loss: 27.6347\n",
      "Train Epoch: 182 [0/60000 (0%)]\tLoss: 26.766531\n",
      "Train Epoch: 182 [20000/60000 (33%)]\tLoss: 26.422317\n",
      "Train Epoch: 182 [40000/60000 (67%)]\tLoss: 29.995774\n",
      ">> Epoch: 182 Average loss: 27.0705\n",
      ">> Test set loss: 27.7295\n",
      "Train Epoch: 183 [0/60000 (0%)]\tLoss: 26.510806\n",
      "Train Epoch: 183 [20000/60000 (33%)]\tLoss: 28.785771\n",
      "Train Epoch: 183 [40000/60000 (67%)]\tLoss: 29.110901\n",
      ">> Epoch: 183 Average loss: 27.0751\n",
      ">> Test set loss: 27.5121\n",
      "Train Epoch: 184 [0/60000 (0%)]\tLoss: 28.316475\n",
      "Train Epoch: 184 [20000/60000 (33%)]\tLoss: 27.022764\n",
      "Train Epoch: 184 [40000/60000 (67%)]\tLoss: 26.149265\n",
      ">> Epoch: 184 Average loss: 26.9925\n",
      ">> Test set loss: 27.4053\n",
      "Train Epoch: 185 [0/60000 (0%)]\tLoss: 26.354888\n",
      "Train Epoch: 185 [20000/60000 (33%)]\tLoss: 25.730142\n",
      "Train Epoch: 185 [40000/60000 (67%)]\tLoss: 24.936382\n",
      ">> Epoch: 185 Average loss: 26.9059\n",
      ">> Test set loss: 27.4238\n",
      "Train Epoch: 186 [0/60000 (0%)]\tLoss: 25.347947\n",
      "Train Epoch: 186 [20000/60000 (33%)]\tLoss: 27.892031\n",
      "Train Epoch: 186 [40000/60000 (67%)]\tLoss: 27.919451\n",
      ">> Epoch: 186 Average loss: 26.9411\n",
      ">> Test set loss: 27.2976\n",
      "Train Epoch: 187 [0/60000 (0%)]\tLoss: 27.617646\n",
      "Train Epoch: 187 [20000/60000 (33%)]\tLoss: 26.851008\n",
      "Train Epoch: 187 [40000/60000 (67%)]\tLoss: 27.167864\n",
      ">> Epoch: 187 Average loss: 26.8023\n",
      ">> Test set loss: 27.3571\n",
      "Train Epoch: 188 [0/60000 (0%)]\tLoss: 28.256960\n",
      "Train Epoch: 188 [20000/60000 (33%)]\tLoss: 26.061997\n",
      "Train Epoch: 188 [40000/60000 (67%)]\tLoss: 26.518008\n",
      ">> Epoch: 188 Average loss: 26.7959\n",
      ">> Test set loss: 27.2420\n",
      "Train Epoch: 189 [0/60000 (0%)]\tLoss: 27.267590\n",
      "Train Epoch: 189 [20000/60000 (33%)]\tLoss: 27.106982\n",
      "Train Epoch: 189 [40000/60000 (67%)]\tLoss: 28.504434\n",
      ">> Epoch: 189 Average loss: 26.7553\n",
      ">> Test set loss: 27.3986\n",
      "Train Epoch: 190 [0/60000 (0%)]\tLoss: 27.815442\n",
      "Train Epoch: 190 [20000/60000 (33%)]\tLoss: 28.048262\n",
      "Train Epoch: 190 [40000/60000 (67%)]\tLoss: 26.631125\n",
      ">> Epoch: 190 Average loss: 26.8741\n",
      ">> Test set loss: 27.3006\n",
      "Train Epoch: 191 [0/60000 (0%)]\tLoss: 26.817717\n",
      "Train Epoch: 191 [20000/60000 (33%)]\tLoss: 26.655474\n",
      "Train Epoch: 191 [40000/60000 (67%)]\tLoss: 27.904875\n",
      ">> Epoch: 191 Average loss: 26.8649\n",
      ">> Test set loss: 27.3316\n",
      "Train Epoch: 192 [0/60000 (0%)]\tLoss: 26.298059\n",
      "Train Epoch: 192 [20000/60000 (33%)]\tLoss: 25.866340\n",
      "Train Epoch: 192 [40000/60000 (67%)]\tLoss: 26.344719\n",
      ">> Epoch: 192 Average loss: 26.9682\n",
      ">> Test set loss: 27.3511\n",
      "Train Epoch: 193 [0/60000 (0%)]\tLoss: 27.855410\n",
      "Train Epoch: 193 [20000/60000 (33%)]\tLoss: 29.291978\n",
      "Train Epoch: 193 [40000/60000 (67%)]\tLoss: 26.141001\n",
      ">> Epoch: 193 Average loss: 26.9294\n",
      ">> Test set loss: 27.8333\n",
      "Train Epoch: 194 [0/60000 (0%)]\tLoss: 28.719111\n",
      "Train Epoch: 194 [20000/60000 (33%)]\tLoss: 24.901465\n",
      "Train Epoch: 194 [40000/60000 (67%)]\tLoss: 27.560400\n",
      ">> Epoch: 194 Average loss: 27.0595\n",
      ">> Test set loss: 27.3200\n",
      "Train Epoch: 195 [0/60000 (0%)]\tLoss: 26.708110\n",
      "Train Epoch: 195 [20000/60000 (33%)]\tLoss: 26.508877\n",
      "Train Epoch: 195 [40000/60000 (67%)]\tLoss: 27.921951\n",
      ">> Epoch: 195 Average loss: 26.8516\n",
      ">> Test set loss: 27.2622\n",
      "Train Epoch: 196 [0/60000 (0%)]\tLoss: 26.629810\n",
      "Train Epoch: 196 [20000/60000 (33%)]\tLoss: 25.138645\n",
      "Train Epoch: 196 [40000/60000 (67%)]\tLoss: 26.877402\n",
      ">> Epoch: 196 Average loss: 26.7895\n",
      ">> Test set loss: 27.2797\n",
      "Train Epoch: 197 [0/60000 (0%)]\tLoss: 26.287947\n",
      "Train Epoch: 197 [20000/60000 (33%)]\tLoss: 26.110088\n",
      "Train Epoch: 197 [40000/60000 (67%)]\tLoss: 24.765979\n",
      ">> Epoch: 197 Average loss: 26.8720\n",
      ">> Test set loss: 27.3682\n",
      "Train Epoch: 198 [0/60000 (0%)]\tLoss: 26.859629\n",
      "Train Epoch: 198 [20000/60000 (33%)]\tLoss: 24.975984\n",
      "Train Epoch: 198 [40000/60000 (67%)]\tLoss: 27.521528\n",
      ">> Epoch: 198 Average loss: 26.8858\n",
      ">> Test set loss: 27.3619\n",
      "Train Epoch: 199 [0/60000 (0%)]\tLoss: 27.480554\n",
      "Train Epoch: 199 [20000/60000 (33%)]\tLoss: 29.476230\n",
      "Train Epoch: 199 [40000/60000 (67%)]\tLoss: 27.465688\n",
      ">> Epoch: 199 Average loss: 26.9534\n",
      ">> Test set loss: 27.2376\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100, 200):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        # sample = torch.randn(64, 2).to(device)\n",
    "        sample = space + .1 * torch.randn(16 * 16, 2).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        utils.save_image(sample.view(16 * 16, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png', nrow=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:24:58.889032Z",
     "start_time": "2018-11-12T21:24:58.853567Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoEncoder(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=12, bias=True)\n",
      "  (fc41): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (fc42): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (fc5): Linear(in_features=2, out_features=12, bias=True)\n",
      "  (fc6): Linear(in_features=12, out_features=50, bias=True)\n",
      "  (fc7): Linear(in_features=50, out_features=128, bias=True)\n",
      "  (fc8): Linear(in_features=128, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vae = autoencoders.VariationalAutoEncoder()\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:00.741088Z",
     "start_time": "2018-11-12T21:25:00.713061Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:01.061400Z",
     "start_time": "2018-11-12T21:25:01.035738Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:01.502154Z",
     "start_time": "2018-11-12T21:25:01.473418Z"
    }
   },
   "outputs": [],
   "source": [
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:02.303174Z",
     "start_time": "2018-11-12T21:25:02.273408Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:03.539087Z",
     "start_time": "2018-11-12T21:25:03.510430Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:04.962635Z",
     "start_time": "2018-11-12T21:25:04.916490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function_vae(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_variance = vae(data)\n",
    "        loss = loss_function_vae(recon_batch, data, mu, log_variance)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % log_interval == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #         100. * batch_idx / len(train_loader),\n",
    "        #         loss.item() / len(data)))\n",
    "\n",
    "    print('>> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    writer.add_scalar('data/train-loss', train_loss / len(train_loader.dataset), epoch)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    vae.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, log_variance = vae(data)\n",
    "            test_loss += loss_function_vae(recon_batch, data, mu, log_variance).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(100, 1, 28, 28)[:n]])\n",
    "                # utils.save_image(comparison.cpu(),\n",
    "                #          'results/vae-reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "                \n",
    "                writer.add_image('reconstruction', comparison.cpu(), epoch)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('>> Test set loss: {:.4f}'.format(test_loss))\n",
    "    \n",
    "    writer.add_scalar('data/test-loss', test_loss, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:06.015840Z",
     "start_time": "2018-11-12T21:25:05.990238Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:25:06.477661Z",
     "start_time": "2018-11-12T21:25:06.450224Z"
    }
   },
   "outputs": [],
   "source": [
    "space = np.array([[x, y] for y in np.linspace(-1.5, 1.5, 16) for x in np.linspace(-1.5, 1.5, 16)], dtype=np.float)\n",
    "space = torch.from_numpy(space).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:35:40.324862Z",
     "start_time": "2018-11-12T21:25:08.700960Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch: 1 Average loss: 207.4965\n",
      ">> Test set loss: 183.7865\n",
      ">> Epoch: 2 Average loss: 173.9688\n",
      ">> Test set loss: 168.5135\n",
      ">> Epoch: 3 Average loss: 165.8932\n",
      ">> Test set loss: 163.6707\n",
      ">> Epoch: 4 Average loss: 161.1973\n",
      ">> Test set loss: 159.0849\n",
      ">> Epoch: 5 Average loss: 157.4966\n",
      ">> Test set loss: 156.1624\n",
      ">> Epoch: 6 Average loss: 155.0789\n",
      ">> Test set loss: 154.2824\n",
      ">> Epoch: 7 Average loss: 153.4772\n",
      ">> Test set loss: 153.5132\n",
      ">> Epoch: 8 Average loss: 152.2615\n",
      ">> Test set loss: 151.9901\n",
      ">> Epoch: 9 Average loss: 151.1153\n",
      ">> Test set loss: 151.1691\n",
      ">> Epoch: 10 Average loss: 150.1952\n",
      ">> Test set loss: 150.4310\n",
      ">> Epoch: 11 Average loss: 149.4633\n",
      ">> Test set loss: 149.4782\n",
      ">> Epoch: 12 Average loss: 148.7509\n",
      ">> Test set loss: 148.9511\n",
      ">> Epoch: 13 Average loss: 148.2019\n",
      ">> Test set loss: 148.4081\n",
      ">> Epoch: 14 Average loss: 147.6030\n",
      ">> Test set loss: 147.8209\n",
      ">> Epoch: 15 Average loss: 147.0881\n",
      ">> Test set loss: 147.1794\n",
      ">> Epoch: 16 Average loss: 146.7729\n",
      ">> Test set loss: 147.1360\n",
      ">> Epoch: 17 Average loss: 146.2783\n",
      ">> Test set loss: 147.3422\n",
      ">> Epoch: 18 Average loss: 145.8711\n",
      ">> Test set loss: 146.0674\n",
      ">> Epoch: 19 Average loss: 145.5999\n",
      ">> Test set loss: 146.2659\n",
      ">> Epoch: 20 Average loss: 145.2783\n",
      ">> Test set loss: 145.8625\n",
      ">> Epoch: 21 Average loss: 145.0581\n",
      ">> Test set loss: 145.4693\n",
      ">> Epoch: 22 Average loss: 144.7953\n",
      ">> Test set loss: 145.2110\n",
      ">> Epoch: 23 Average loss: 144.5620\n",
      ">> Test set loss: 144.8738\n",
      ">> Epoch: 24 Average loss: 144.3763\n",
      ">> Test set loss: 144.8924\n",
      ">> Epoch: 25 Average loss: 144.0890\n",
      ">> Test set loss: 144.8058\n",
      ">> Epoch: 26 Average loss: 144.0218\n",
      ">> Test set loss: 144.2412\n",
      ">> Epoch: 27 Average loss: 143.4410\n",
      ">> Test set loss: 144.0983\n",
      ">> Epoch: 28 Average loss: 143.5106\n",
      ">> Test set loss: 144.6359\n",
      ">> Epoch: 29 Average loss: 143.4745\n",
      ">> Test set loss: 143.9787\n",
      ">> Epoch: 30 Average loss: 143.2313\n",
      ">> Test set loss: 143.7861\n",
      ">> Epoch: 31 Average loss: 142.9069\n",
      ">> Test set loss: 143.3718\n",
      ">> Epoch: 32 Average loss: 142.8318\n",
      ">> Test set loss: 143.8705\n",
      ">> Epoch: 33 Average loss: 142.6846\n",
      ">> Test set loss: 143.1527\n",
      ">> Epoch: 34 Average loss: 142.3055\n",
      ">> Test set loss: 142.9630\n",
      ">> Epoch: 35 Average loss: 142.5104\n",
      ">> Test set loss: 143.3771\n",
      ">> Epoch: 36 Average loss: 142.4555\n",
      ">> Test set loss: 142.9966\n",
      ">> Epoch: 37 Average loss: 142.2049\n",
      ">> Test set loss: 143.1865\n",
      ">> Epoch: 38 Average loss: 141.9380\n",
      ">> Test set loss: 142.6656\n",
      ">> Epoch: 39 Average loss: 141.6554\n",
      ">> Test set loss: 143.0443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-285ab3081d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# sample = torch.randn(64, 2).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5d04b46afb90>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours/PGM/Project/autoencoders/autoencoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours/PGM/Project/autoencoders/autoencoders.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_flat_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        # sample = torch.randn(64, 2).to(device)\n",
    "        sample = space # + .05 * torch.randn(16 * 16, 2).to(device)\n",
    "        sample = vae.decode(sample).cpu()\n",
    "        utils.save_image(sample.view(16 * 16, 1, 28, 28),\n",
    "                   'results/vae-sample_' + str(epoch) + '.png', nrow=16)\n",
    "        writer.add_image('sample', utils.make_grid(sample.view(16 * 16, 1, 28, 28), nrow=16), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
